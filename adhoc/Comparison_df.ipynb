{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, HiveContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_content(url): \n",
    "    \"\"\"Method to download the contents from webpage\"\"\"\n",
    "    req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()    \n",
    "    page_soup = soup(webpage, \"html.parser\")\n",
    "    containers = page_soup.findAll(\"div\",\"padded\")\n",
    "    for container in containers:\n",
    "        hyperlinks=container.findAll('a')\n",
    "        for hyperlink in hyperlinks:\n",
    "            download_url=hyperlink.get('href')\n",
    "            if download_url.endswith('file_name pattern'):\n",
    "                download_url='url'+download_url\n",
    "                print (download_url)             \n",
    "                r = requests.get(download_url, stream = True)\n",
    "                file_name='file_name.pdf'\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size = 1024*1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            print (\"file downloaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'url_input'\n",
    "download_content(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"schema Comaprsion Df\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.instances\",\"10\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_db=spark.sql('use cm2506')\n",
    "show_info=spark.sql(\"desc cm2506.target_tbl\")\n",
    "show_info.registerTempTable('cmp_target_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df1=spark.sql(\"select col_name,data_type from cmp_target_tbl\")\n",
    "print(type(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows extarcted : 26 \n",
      "Number of cols extarcted : 6 \n"
     ]
    }
   ],
   "source": [
    "input='/home/cm2506/Documents/data/input.xlsx'\n",
    "loc = (input) \n",
    "  \n",
    "wb = xlrd.open_workbook(loc) \n",
    "sheet = wb.sheet_by_index(0) \n",
    "sheet.cell_value(0, 0) \n",
    "  \n",
    "rows='Number of rows extarcted : {0} '.format(sheet.nrows)\n",
    "print(rows)\n",
    "cols='Number of cols extarcted : {0} '.format(sheet.ncols)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "column_list=[]\n",
    "type_list=[]\n",
    "for i in range(sheet.ncols): \n",
    "    col_name=sheet.cell_value(2, i)\n",
    "    if col_name =='column_name':\n",
    "        column_name=col_name\n",
    "        for j in range(sheet.nrows):\n",
    "            #print(sheet.cell_value(j, 4)) \n",
    "            if j >=3:                \n",
    "                col_value=str(sheet.cell_value(j, 4)).lower()\n",
    "                column_list.append(col_value)            \n",
    "    if col_name == 'column_type':\n",
    "        column_type=col_name\n",
    "        for j in range(sheet.nrows):\n",
    "            #print(sheet.cell_value(j, 5))           \n",
    "            if j >=3:\n",
    "                type_list.append(sheet.cell_value(j, 5))\n",
    "\n",
    "d= {'col_name': column_list,'data_type':type_list}\n",
    "#print(d)\n",
    "df = pd.DataFrame(d, columns = ['col_name', 'data_type'])\n",
    "print (type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "pdf = spark.createDataFrame(df)\n",
    "print(type(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+\n",
      "|         col_name|data_type|\n",
      "+-----------------+---------+\n",
      "|         geocode |   string|\n",
      "|        geopoint |   string|\n",
      "|        cbsacode |   string|\n",
      "| cbsadivisioncode|   string|\n",
      "|cbsadivisionlevel|   string|\n",
      "|cbsadivisiontitle|   string|\n",
      "|       cbsalevel |   string|\n",
      "|       cbsatitle |   string|\n",
      "|      censusblock|   string|\n",
      "|      censustract|   string|\n",
      "|       countyfips|   string|\n",
      "|       countyname|   string|\n",
      "|       errorcode |   string|\n",
      "|         latitude|   string|\n",
      "|        longitude|   string|\n",
      "|       placecode |   string|\n",
      "|       placename |   string|\n",
      "|      statuscode |   string|\n",
      "|         timezone|   string|\n",
      "|     timezonecode|   string|\n",
      "+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.registerTempTable('cmp_source_tbl')\n",
    "df2=spark.sql(\"select col_name,data_type from cmp_source_tbl\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp=spark.sql('select a.*,case when a.col_name=b.col_name then \"Matched\" else \"Unmatched\" end as matching_ind from cmp_source_tbl a join cmp_target_tbl b on a.col_name = b.col_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------------+\n",
      "|         col_name|data_type|matching_ind|\n",
      "+-----------------+---------+------------+\n",
      "| cbsadivisioncode|   string|     Matched|\n",
      "|cbsadivisionlevel|   string|     Matched|\n",
      "|cbsadivisiontitle|   string|     Matched|\n",
      "|      censusblock|   string|     Matched|\n",
      "|      censustract|   string|     Matched|\n",
      "|       countyfips|   string|     Matched|\n",
      "|       countyname|   string|     Matched|\n",
      "|         latitude|   string|     Matched|\n",
      "|        longitude|   string|     Matched|\n",
      "|         timezone|   string|     Matched|\n",
      "|     timezonecode|   string|     Matched|\n",
      "|         distance|   string|     Matched|\n",
      "|          geohash|   string|     Matched|\n",
      "+-----------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp.toPandas().to_csv('output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
